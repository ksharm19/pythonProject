from pyspark import SparkContext

sc = SparkContext("local[*]", "Movie-Data")
a = range(1,101)
base = sc.parallelize(a)accu
result = base.reduce(lambda x,y: x+y)

print(result)


create index indexnbame on employee (col1,col2)

select rn* from

(select salary as sal,
row_num (partionby sal order by title)rn)

where

11
11111
\
sal jobtitle
emp name, empid,sal, jobtitle

de, dir(
    avg salary for each job title and populate for all 10 rows


)


num = 11

if num > 1:
#iterate on the range
#number should be divisible by 2

for i inrange(num/2):
    print("even")


def pal(s):
    rev = ''.join(reverse(s))


AWS S3 JSOOn we are making cal pull request call by using tool iics load into Staging

2n pipeline In IISC expression(tranformation) where we are firstname lastname
3rd expression we can add newcol "Ingestion data"
load this into target


x = laoding the file()

rdd1 = x.map(x=> split(",")
rdd2 = x.flatMap(x,y = x + y )

rdd3 = x.count(x)

rdd3.collect







